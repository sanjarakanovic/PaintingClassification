{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89Ahirejkyty"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import scipy.io as sc\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.applications.efficientnet import preprocess_input\n",
        "from keras.applications import EfficientNetB0\n",
        "from keras.layers import Dense, GlobalAveragePooling2D\n",
        "from keras.models import Model\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "\n",
        "IMAGE_SIZE = (224,224)\n",
        "BATCH_SIZE = 64\n",
        "NUM_OF_CLASSES = 13\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "ROOT_DIR = 'Paintings91/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jB8KLFREHRM"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')\n",
        "!unzip drive/MyDrive/Paintings91.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWcBsdSUDhS7"
      },
      "outputs": [],
      "source": [
        "def preprocess_image(filename, label):\n",
        "\n",
        "    image = tf.io.read_file(ROOT_DIR + 'Images/' + filename)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, IMAGE_SIZE)\n",
        "    image = preprocess_input(image)\n",
        "\n",
        "    return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8o2nSbYCOTF"
      },
      "outputs": [],
      "source": [
        "def get_data_sets():\n",
        "\n",
        "  dir_suffix = \"\"\n",
        "  file_suffix = \"\"\n",
        "\n",
        "  if(NUM_OF_CLASSES == 13):\n",
        "    dir_suffix = \"_Style\"\n",
        "    file_suffix = \"_style\"\n",
        "\n",
        "  labels = sc.loadmat(file_name = ROOT_DIR +\"/Labels\" + dir_suffix + \"/labels\"+ file_suffix + \".mat\", mdict=None, appendmat=True)\n",
        "  testset = sc.loadmat(file_name = ROOT_DIR +\"/Labels\" + dir_suffix + \"/testset\" + file_suffix + \".mat\", mdict=None, appendmat=True)\n",
        "  trainset = sc.loadmat(file_name = ROOT_DIR +\"/Labels\" + dir_suffix + \"/trainset\" + file_suffix + \".mat\", mdict=None, appendmat=True)\n",
        "  image_names = sc.loadmat(file_name = ROOT_DIR +\"/Labels\" + dir_suffix +\"/image_names\" + file_suffix + \".mat\", mdict=None, appendmat=True)\n",
        "\n",
        "  image_names['image_names' + file_suffix] = np.array(\n",
        "    list(map(lambda name: name[0][0].replace('Ã', 'Р'),\n",
        "             image_names['image_names' + file_suffix ])))\n",
        "\n",
        "  train_indices = np.where(trainset['trainset' + file_suffix].ravel())\n",
        "  test_indices = np.where(testset['testset' + file_suffix].ravel())\n",
        "\n",
        "  X_train = image_names['image_names' + file_suffix][train_indices]\n",
        "  y_train = labels['labels' + file_suffix][train_indices]\n",
        "\n",
        "  X_test = image_names['image_names' + file_suffix][test_indices]\n",
        "  y_test = labels['labels' + file_suffix][test_indices]\n",
        "\n",
        "  X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, stratify = y_train)\n",
        "\n",
        "  train_ds =  tf.data.Dataset.from_tensor_slices((X_train,y_train)).map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
        "  val_ds =  tf.data.Dataset.from_tensor_slices((X_val,y_val)).map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
        "  test_ds =  tf.data.Dataset.from_tensor_slices((X_test,y_test)).map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
        "\n",
        "  return train_ds, val_ds, test_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_XCjBUbLmB1"
      },
      "outputs": [],
      "source": [
        "train_ds, val_ds, test_ds = get_data_sets()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-KpaQW2Oi_H"
      },
      "outputs": [],
      "source": [
        "base_model = EfficientNetB0(weights='imagenet', include_top=False)\n",
        "x = base_model.output\n",
        "for layer in base_model.layers[:213]:\n",
        "  layer.trainable = False\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "output = Dense(NUM_OF_CLASSES, activation='softmax')(x)\n",
        "model = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "model.compile(optimizer = Adam(1e-3), loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1), metrics=['accuracy'])\n",
        "\n",
        "early_stopping = EarlyStopping(monitor = 'val_accuracy', patience = 8, restore_best_weights = True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.2, patience = 5, min_lr = 1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99vFS00iOqjS"
      },
      "outputs": [],
      "source": [
        "model.fit(train_ds, validation_data=(val_ds), callbacks=[early_stopping, reduce_lr], epochs = 50)"
      ]
    }
  ]
}